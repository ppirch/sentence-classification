{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence Classification with Pytorch"
      ],
      "metadata": {
        "id": "ETgOP__keRKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "3ymhBefseWPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q 'transformers[torch]' sentencepiece"
      ],
      "metadata": {
        "id": "9UlCcKfbeVyw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ],
      "metadata": {
        "id": "bz_Pzy4Qejko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, AutoModel"
      ],
      "metadata": {
        "id": "TnMyps04eUMX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Training and testing dataset"
      ],
      "metadata": {
        "id": "IgKw_TDHevMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzwEkPiJe9HI",
        "outputId": "a1194bcf-5702-4e91-bfa5-e0571f62f598"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_TEXT_URL = \"https://raw.githubusercontent.com/PyThaiNLP/wisesight-sentiment/master/kaggle-competition/train.txt\"\n",
        "TRAIN_LABEL_URL = \"https://raw.githubusercontent.com/PyThaiNLP/wisesight-sentiment/master/kaggle-competition/train_label.txt\"\n",
        "TEST_TEXT_URL = \"https://raw.githubusercontent.com/PyThaiNLP/wisesight-sentiment/master/kaggle-competition/test.txt\"\n",
        "TEST_LABEL_URL = \"https://raw.githubusercontent.com/PyThaiNLP/wisesight-sentiment/master/kaggle-competition/test_label.txt\"\n",
        "\n",
        "train_text = requests.get(TRAIN_TEXT_URL).text\n",
        "train_label = requests.get(TRAIN_LABEL_URL).text\n",
        "test_text = requests.get(TEST_TEXT_URL).text\n",
        "test_label = requests.get(TEST_LABEL_URL).text\n",
        "\n",
        "train_df = pd.DataFrame(\n",
        "    {\"text\": train_text.split(\"\\n\")[:-1], \"label\": train_label.split(\"\\n\")[:-1]}\n",
        ")\n",
        "\n",
        "test_df = pd.DataFrame(\n",
        "    {\"text\": test_text.split(\"\\n\")[:-1], \"label\": test_label.split(\"\\n\")[:-1]}\n",
        ")\n",
        "\n",
        "train_df.to_csv(\"data/train.csv\", index=False)\n",
        "test_df.to_csv(\"data/test.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "zuILVWFJeuqz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Pytorch Dataset"
      ],
      "metadata": {
        "id": "MePpvhyLfA5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, df: pd.DataFrame, label2index: Dict[str, int], tokenizer: AutoTokenizer):\n",
        "    self.labels = [label2index[label] for label in df[\"label\"]]\n",
        "    self.texts = [\n",
        "      tokenizer(\n",
        "        text,\n",
        "        padding=\"max_length\",\n",
        "        max_length=416,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "      )\n",
        "      for text in df[\"text\"]\n",
        "    ]\n",
        "\n",
        "  def classes(self):\n",
        "    return self.labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def get_batch_labels(self, idx):\n",
        "    return np.array(self.labels[idx])\n",
        "\n",
        "  def get_batch_texts(self, idx):\n",
        "    return self.texts[idx]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    batch_texts = self.get_batch_texts(idx)\n",
        "    batch_y = self.get_batch_labels(idx)\n",
        "    return batch_texts, batch_y"
      ],
      "metadata": {
        "id": "XWsL4mE9e6m9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Classifier"
      ],
      "metadata": {
        "id": "G5AeBESBfoij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WangchanBERTaClassifier(nn.Module):\n",
        "  def __init__(self, model_name: str, num_classes: int = 4, dropout: float = 0.5):\n",
        "    super(WangchanBERTaClassifier, self).__init__()\n",
        "    self.bert = AutoModel.from_pretrained(model_name)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear = nn.Linear(768, num_classes)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, input_id, mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_id, attention_mask=mask, return_dict=False\n",
        "    )\n",
        "    dropout_output = self.dropout(pooled_output)\n",
        "    linear_output = self.linear(dropout_output)\n",
        "    final_layer = self.relu(linear_output)\n",
        "    return final_layer"
      ],
      "metadata": {
        "id": "2GsvgpEgfnr-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Training Function"
      ],
      "metadata": {
        "id": "-DublBv6f1UL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_dataloader, val_dataloader, learning_rate, epochs):\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  if use_cuda:\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "\n",
        "  for epoch_num in range(epochs):\n",
        "    total_acc_train = 0\n",
        "    total_loss_train = 0\n",
        "\n",
        "    for train_input, train_label in tqdm(train_dataloader):\n",
        "      train_label = train_label.to(device)\n",
        "      mask = train_input[\"attention_mask\"].to(device)\n",
        "      input_id = train_input[\"input_ids\"].squeeze(1).to(device)\n",
        "\n",
        "      output = model(input_id, mask)\n",
        "\n",
        "      batch_loss = criterion(output, train_label.long())\n",
        "      total_loss_train += batch_loss.item()\n",
        "\n",
        "      acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "      total_acc_train += acc\n",
        "\n",
        "      model.zero_grad()\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    total_acc_val = 0\n",
        "    total_loss_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for val_input, val_label in val_dataloader:\n",
        "        val_label = val_label.to(device)\n",
        "        mask = val_input[\"attention_mask\"].to(device)\n",
        "        input_id = val_input[\"input_ids\"].squeeze(1).to(device)\n",
        "\n",
        "        output = model(input_id, mask)\n",
        "\n",
        "        batch_loss = criterion(output, val_label.long())\n",
        "        total_loss_val += batch_loss.item()\n",
        "\n",
        "        acc = (output.argmax(dim=1) == val_label).sum().item()\n",
        "        total_acc_val += acc\n",
        "\n",
        "    print(\n",
        "      f\"Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_dataloader.dataset): .3f} \\\n",
        "        | Train Accuracy: {total_acc_train / len(train_dataloader.dataset): .3f} \\\n",
        "        | Val Loss: {total_loss_val / len(val_dataloader.dataset): .3f} \\\n",
        "        | Val Accuracy: {total_acc_val / len(val_dataloader.dataset): .3f}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "czm96Ho0fz2K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Evaluating Function"
      ],
      "metadata": {
        "id": "sgtJgPjdgInK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_dataloader):\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "  if use_cuda:\n",
        "    model = model.cuda()\n",
        "\n",
        "  total_acc_test = 0\n",
        "  with torch.no_grad():\n",
        "    for test_input, test_label in test_dataloader:\n",
        "      test_label = test_label.to(device)\n",
        "      mask = test_input[\"attention_mask\"].to(device)\n",
        "      input_id = test_input[\"input_ids\"].squeeze(1).to(device)\n",
        "\n",
        "      output = model(input_id, mask)\n",
        "\n",
        "      acc = (output.argmax(dim=1) == test_label).sum().item()\n",
        "      total_acc_test += acc\n",
        "\n",
        "  print(f\"Test Accuracy: {total_acc_test / len(test_dataloader.dataset): .3f}\")"
      ],
      "metadata": {
        "id": "QlzcmNgLgH7t"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting"
      ],
      "metadata": {
        "id": "o_kHzn7ygYw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(677)\n",
        "np.random.seed(677)\n",
        "\n",
        "label2index = {\n",
        "  \"pos\": 0,\n",
        "  \"neu\": 1,\n",
        "  \"neg\": 2,\n",
        "  \"q\": 3,\n",
        "}\n",
        "\n",
        "MODEL_NAME = \"airesearch/wangchanBERTa-base-att-spm-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "model = WangchanBERTaClassifier(MODEL_NAME, 4)\n",
        "EPOCHS = 5\n",
        "LR = 1e-6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWtxkef5gWjN",
        "outputId": "12f9c665-3192-47f3-8f82-298716075427"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at airesearch/wangchanBERTa-base-att-spm-uncased were not used when initializing CamembertModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read data"
      ],
      "metadata": {
        "id": "AoEQQ3EthZfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"data/train.csv\")\n",
        "\n",
        "df_train, df_val = np.split(\n",
        "    df.sample(frac=1, random_state=42), [int(0.8 * len(df))]\n",
        ")\n",
        "print(len(df_train), len(df_val))\n",
        "\n",
        "train_dataset, val_dataset = Dataset(df_train, label2index, tokenizer), Dataset(df_val, label2index, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XSSE80IhZAs",
        "outputId": "fb556794-4cc6-493f-baba-a978936f8af1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19250 4813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "NpvvAm8xhjgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=16)\n",
        "train(model, train_dataloader, val_dataloader, LR, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie_jF8L6hiel",
        "outputId": "8b60fb70-14cf-4cb1-ab4a-98ed30c06793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 13/602 [00:28<21:32,  2.19s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test model"
      ],
      "metadata": {
        "id": "DphbKPLbiAQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv(\"data/test.csv\")\n",
        "test_dataset = Dataset(df_test, label2index, tokenizer)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n",
        "evaluate(model, test_dataloader)"
      ],
      "metadata": {
        "id": "TrastEFph_ur"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}